{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 1\n",
    "**\n",
    "Chinnu wants to help his sister pravallika to pass through the aptitude exam, so in order to help her he wants to test her skills in english .So he decided to assign some sentences and want the key root words in the sentences. So implement a python code that helps pravallika to get the root words in the given sentences?(Note:Use stemming  and tokenization process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pintu', 'is', 'using', 'a', 'computer']\n",
      "Pintu  :  pintu\n",
      "is  :  is\n",
      "using  :  use\n",
      "a  :  a\n",
      "computer  :  comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "sentence = \"Pintu is using a computer\"\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 2\n",
    "\n",
    "Chinnu is a comic editor in a X company one day while editing a particular script he became enthusiastic about the story of the script so there is no time to read the whole script he decided to understand the total story line by learning about the characters so he wants to separate the words in the sentence to know the characters as the whole story is complex ,So implement a python program that splits the words and  display both splitted words and count of the words in the given sentence using  tokenizer  function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chintu', 'is', 'very', 'very', 'stupid']\n",
      "The number of words in string are : 5\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    " \n",
    "sentence = \"Chintu is very very stupid\"\n",
    "words = word_tokenize(sentence) \n",
    "print(words)\n",
    "res = len(sentence.split()) \n",
    "print (\"The number of words in string are : \" +  str(res)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Lab 1\n",
    "\n",
    "Albert is a English teacher in a school.He wants to teach the students about connectors and prepositions and there usage in sentence formation so he thought an idea that helps students to understand more about the connectors and prepositions so he planned to give different sentences to the students and to remove the connectors and prepositions in the sentence .Implement a python code to help students to remove those connectors and prepositions?(Note: connectors and prepositions represents stop words take them in a text file for required output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Example', 'sentence', ',', 'example']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords       #importing stopwords (connectives or prepositions)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"Example sentence, this is an example\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word = word_tokenize(example)\n",
    "\n",
    "filtered = [w for w in word if not w in stop_words]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 1\n",
    "\n",
    "Dany is struggling with her assignment given by her tuition master. She has to read a paragraph and generate the tokens from the paragraph using sentence tokenizer. She should also find the parts of speech for each word in the individual tokens that she has generated using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIRAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KIRAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Notice of a bid advertisement shall be published in at least one local newspaper and in one trade publication at least 30 days in advance of sale.', 'If applicable, the notice must identify the reservation within which the tracts to be leased are found.', 'Specific descriptions of the tracts shall be available at the office of the superintendent.', 'The complete text of the advertisement shall be mailed to each person listed on the appropriate agency mailing list.']\n",
      "Iteration  0\n",
      "Sentence = Notice of a bid advertisement shall be published in at least one local newspaper and in one trade publication at least 30 days in advance of sale.\n",
      "[('Notice', 'NNP'), ('of', 'IN'), ('a', 'DT'), ('bid', 'NN'), ('advertisement', 'NN'), ('shall', 'MD'), ('be', 'VB'), ('published', 'VBN'), ('in', 'IN'), ('at', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('local', 'JJ'), ('newspaper', 'NN'), ('and', 'CC'), ('in', 'IN'), ('one', 'CD'), ('trade', 'NN'), ('publication', 'NN'), ('at', 'IN'), ('least', 'JJS'), ('30', 'CD'), ('days', 'NNS'), ('in', 'IN'), ('advance', 'NN'), ('of', 'IN'), ('sale', 'NN'), ('.', '.')]\n",
      "Iteration  1\n",
      "Sentence = If applicable, the notice must identify the reservation within which the tracts to be leased are found.\n",
      "[('If', 'IN'), ('applicable', 'JJ'), (',', ','), ('the', 'DT'), ('notice', 'NN'), ('must', 'MD'), ('identify', 'VB'), ('the', 'DT'), ('reservation', 'NN'), ('within', 'IN'), ('which', 'WDT'), ('the', 'DT'), ('tracts', 'NNS'), ('to', 'TO'), ('be', 'VB'), ('leased', 'VBN'), ('are', 'VBP'), ('found', 'VBN'), ('.', '.')]\n",
      "Iteration  2\n",
      "Sentence = Specific descriptions of the tracts shall be available at the office of the superintendent.\n",
      "[('Specific', 'JJ'), ('descriptions', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('tracts', 'NNS'), ('shall', 'MD'), ('be', 'VB'), ('available', 'JJ'), ('at', 'IN'), ('the', 'DT'), ('office', 'NN'), ('of', 'IN'), ('the', 'DT'), ('superintendent', 'NN'), ('.', '.')]\n",
      "Iteration  3\n",
      "Sentence = The complete text of the advertisement shall be mailed to each person listed on the appropriate agency mailing list.\n",
      "[('The', 'DT'), ('complete', 'JJ'), ('text', 'NN'), ('of', 'IN'), ('the', 'DT'), ('advertisement', 'NN'), ('shall', 'MD'), ('be', 'VB'), ('mailed', 'VBN'), ('to', 'TO'), ('each', 'DT'), ('person', 'NN'), ('listed', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('appropriate', 'JJ'), ('agency', 'NN'), ('mailing', 'VBG'), ('list', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "txt=\"Notice of a bid advertisement shall be published in at least one local newspaper and in one trade publication at least 30 days in advance of sale. If applicable, the notice must identify the reservation within which the tracts to be leased are found. Specific descriptions of the tracts shall be available at the office of the superintendent. The complete text of the advertisement shall be mailed to each person listed on the appropriate agency mailing list.\"\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "tokenized = sent_tokenize(txt) \n",
    "print(tokenized)\n",
    "j = 0\n",
    "for i in tokenized:\n",
    "    print(\"Iteration \",str(j))\n",
    "    print(\"Sentence =\",i)\n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    j=j+1\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 2\n",
    "\n",
    "Sheena is working on her literature and needs to complete a task that should contain a parse tree that satisfies the rule she is given and should draw the parse tree using python for the given sentence in a required grammar rule using the chunk parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT little/JJ mouse/NN)\n",
      "  ate/VB\n",
      "  (NP the/DT fresh/JJ cheeze/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Download if punkt error\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "\n",
    "s='The little mouse ate the fresh cheeze'\n",
    "s=nltk.pos_tag(word_tokenize(s))\n",
    "\n",
    "chunk_rule=r\"NP:{<DT>?<JJ>*<NN>}\"\n",
    "chunk_parser=nltk.RegexpParser(chunk_rule)\n",
    "\n",
    "res=chunk_parser.parse(s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mary wanted to prepare a list of events that are being held in her country for which she needs the information of name, location, time of the event and name of the organization etc. She is having all the details of the events that are held, all that she need is that the paragraph is modified in such a way that the required details are highlighted in each sentence of the paragraph. Help her out to find the requirements using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KIRAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIRAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\KIRAN/nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7c04f0ac7e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mart_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mart_processed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\KIRAN/nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\KIRAN\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "article = '''Larry Page and Sergey Brin, two students at Stanford University, USA, started BackRub in early 1996.They made it into a company, Google Inc., on September 7, 1998 at a friend's garage in Menlo Park, California.In February 1999, the company moved to 165 University Ave., Palo Alto, California, and then moved to another place called the Googleplex.In September 2001, Google's rating system (PageRank, for saying which information is more helpful) got a U.S. Patent. The patent was to Stanford University, with Lawrence (Larry) Page as the inventor (the person who first had the idea).Google makes a percentage of its money through America Online and InterActiveCorp. It has a special group known as the Partner Solutions Organization (PSO) which helps make contracts, helps making accounts better, and gives engineering help.'''\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "def fn_preprocess(art):\n",
    "    art = nltk.word_tokenize(art)\n",
    "    art = nltk.pos_tag(art)\n",
    "    return art\n",
    "art_processed = fn_preprocess(article)\n",
    "results = ne_chunk(art_processed)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 1 - Paragraph Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "I love drab clothes. Maybe with multiple sentences.\n",
    "\n",
    "And I hate small paragraphs. What about now?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "paragraph_tokenizer = RegexpTokenizer('\\s*\\n\\n\\s*', gaps=True, discard_empty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Keeletehnoloogia on arvutilingvistika praktiline pool. \n",
    "Keeletehnoloogid kasutavad arvutilingvistikas välja töötatud \n",
    "teooriaid, et luua rakendusi (nt arvutiprogramme),mis võimaldavad inimkeelt arvuti abil töödelda ja mõista.Tänapäeval on keeletehnoloogia tuntumateks valdkondadeks masintõlge, arvutileksikoloogia, dialoogisüsteemid,kõneanalüüs ja kõnesüntees.''' \n",
    "tokenizer = Tokenizer() \n",
    "document = tokenizer.tokenize(text) \n",
    "print (document.word_texts) \n",
    "print (document.sentence_texts) \n",
    "print (document.paragraph_texts) \n",
    "print (document.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 2 - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "doc_a = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc_b = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc_c = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_d = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc_e = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "texts = []\n",
    "\n",
    "for i in doc_set:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20)\n",
    "    \n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 1 - Inaugural Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 2 - Conditional Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['president', 'congress']\n",
    "    if w.lower().startswith(target))\n",
    "\n",
    "\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 1\n",
    "\n",
    "Mr. P is very much interested in working with data available in tree bank. So he thought of attending a competition held in Cambridge University with the theme  “Natural Language Processing”. There he is given a set of context free grammar rules and probabilities of non-terminal nodes. He is also given a condition that the parameter values for all rules with the non-terminal nodes on the left-hand side of the rule must sum to one. By satisfying all the above conditions Mr. P is expected to induce PCFG grammar from the tree bank data. Assuming yourself to be Mr. P implement the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG\n",
    "from nltk.corpus import treebank\n",
    "from nltk import treetransforms\n",
    "from nltk import induce_pcfg\n",
    "from nltk.parse import pchart\n",
    "toy_pcfg1 = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "\"\"\")\n",
    "pcfg_prods = toy_pcfg1.productions()\n",
    "pcfg_prod = pcfg_prods[2]\n",
    "print('A PCFG production:', pcfg_prod)\n",
    "print('pcfg_prod.lhs()  =>', pcfg_prod.lhs())\n",
    "print('pcfg_prod.rhs()  =>', pcfg_prod.rhs())\n",
    "print('pcfg_prod.prob() =>', pcfg_prod.prob())\n",
    "\n",
    "print(\"Induce PCFG grammar from treebank data:\")\n",
    "productions = []\n",
    "for item in treebank.fileids()[:2]:\n",
    "    for tree in treebank.parsed_sents(item):\n",
    "        tree.collapse_unary(collapsePOS = False)\n",
    "        tree.chomsky_normal_form(horzMarkov = 2)\n",
    "        productions += tree.productions()\n",
    "from nltk import Nonterminal\n",
    "S = Nonterminal('S')\n",
    "grammar = induce_pcfg(S, productions)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gowtham is a English lecturer so he planned to conduct a exam to all the students in the College.In the exam students have to identify the parts of speech in the given paragraph and as per the evaluation there are few lecturers of English to evaluate all the Students .So in order to complete the evaluation he asked some other lecturers to evaluate and they don’t have enough knowledge regarding the exam requirement . So in order to evaluate properly they thought of using wordlist corpus as it can generate parts of speech. They have to remove stop words from the paragraph and identify the parts of speech in the paragraph. Implement a python code to remove Stopwords and iden1fy parts of speech?\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n",
    "\"Sukanya is geàng married next year. \" \\\n",
    "\"Marriage is a big step in one’s life.\" \\\n",
    "\"It is both exciTIng and frightening. \" \\\n",
    "\"But friendship is a sacred bond between people.\" \\\n",
    "\"It is a special kind of love between us. \" \\\n",
    "\"Many of you must have tried searching for a friend \"\\\n",
    "\"but never found the right one.\"\n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Write down the syntax for the following:\n",
    "1. Import word net, Use the term \"hello\" to find Synsets\n",
    "2. Using Synset find the element in the 0th index, Just the word (using lemmas)\n",
    "3. Name, Definition of that first (0th index) Synset and examples of the word.\n",
    "4. Discern synonyms and antonyms in Synset\n",
    "5. Discern Hypernyms and Hyponyms in Synset **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('hello')[0]\n",
    "print (\"Synset name : \", syn.name())\n",
    "print (\"\\nSynset meaning : \", syn.definition())\n",
    "print (\"\\nSynset example : \", syn.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('h')[0]\n",
    "print (\"Synset name : \", syn.name())\n",
    "print (\"\\nSynset meaning : \", syn.definition())\n",
    "print (\"\\nSynset example : \", syn.examples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"bad\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('hello')[0]\n",
    "print (\"Synset name : \", syn.name())\n",
    "print (\"\\nHypernym : \", syn.hypernyms())\n",
    "print (\"\\nHyponym : \", syn.hypernyms()[0].hyponyms())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 2\n",
    "1. Given two words, calculate the similarity between the words\n",
    "By using Path Similarity\n",
    "By using Wu-Palmer Similarity\n",
    "Word1=car & Word2=bar\n",
    "2. Mention different methods in which we define the distance between words.\n",
    "(i.e other than the methods mentioned above)\n",
    "Answer (2)\n",
    "Levenshtein Distance\n",
    "Longest Common Subsequence Distance Take Away\n",
    "Edit Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "cb = wordnet.synset('car.n.01')\n",
    "ib = wordnet.synset('bar.n.01')\n",
    "cb.path_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn1 = wordnet.synsets('car')[0]\n",
    "syn2 = wordnet.synsets('bar')[0]\n",
    "syn1.wup_similarity(syn2)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 14\n",
    "I took a sentence from The New York Times, “European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices.” \n",
    "a) Import the libraries. \n",
    "b. Then apply word tokenization and part-of-speech tagging to the sentence. \n",
    "c. create a chunk parser and test it on our sentence. \n",
    "d. Identify nationalities or religious or political groups,organization,date and money in the given sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import Tree, pos_tag, ne_chunk\n",
    "sentence = \"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\"\n",
    " \n",
    "def preprocess(sentence):\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    sentence = nltk.pos_tag(sentence)\n",
    "    return sentence\n",
    " \n",
    "def parsing_nltk(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "    chunked = chunkParser.parse(tagged)\n",
    "    print(chunked)\n",
    "def ner(sentence):\n",
    "    ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "    print(ne_tree)\n",
    "\n",
    "print(sentence)\n",
    "print(\"POS TAG OF USING NLTK \")\n",
    "print(preprocess(sentence))\n",
    "print(\"\\n\")\n",
    "print(\"SHALLOW PARSING OF SENTENCE USING NLTK\")\n",
    "print(parsing_nltk(sentence))\n",
    "print(\"\\n\")\n",
    "print(\"NAMED ENTITY RECOGNITION OF SENTENCE USING NLTK\")\n",
    "print(ner(sentence))\n",
    "print(\"-----------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
